<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ricardo Holguin on Ricardo Holguin</title>
    <link>https://ricardohe97.github.io/</link>
    <description>Recent content in Ricardo Holguin on Ricardo Holguin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0700</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sketch2shiba</title>
      <link>https://ricardohe97.github.io/post/sketch2shiba/</link>
      <pubDate>Fri, 09 Nov 2018 11:18:37 -0700</pubDate>
      
      <guid>https://ricardohe97.github.io/post/sketch2shiba/</guid>
      <description>

&lt;h2 id=&#34;antesedenctes&#34;&gt;Antesedenctes&lt;/h2&gt;

&lt;p&gt;Actualmente curso la materia de redes neuronales impartida por el profesor &lt;a href=&#34;http://mat.uson.mx/~juliowaissman/&#34; target=&#34;_blank&#34;&gt;Julio Waissman&lt;/a&gt;, donde para proyecto de mitad de curso teníamos que hacer alguna red neuronal convolucional (CNN). El profesor nos dio la idea de basarnos en algún proyecto o red ya hecha y aplicarla a otra idea que tengamos. También nos mostró algunos artículo o proyectos de &lt;a href=&#34;https://github.com/&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt; que nos pudieran ayudar de inspiración, pero ninguno de los ejemplo que nos mostró me llamó mucho la atención (porque me parecían aburridos o algún otro compañero tomó algún proyecto que también yo quería). Entonces decidí buscar otros proyectos en internet, y fue un poco después de cuando decidí buscar que me encontré con una página de un &lt;a href=&#34;http://cs231n.stanford.edu/project.html&#34; target=&#34;_blank&#34;&gt;curso de Stanford de Redes convolucionales para reconocimiento visual&lt;/a&gt; donde en dicho curso el profesor les dejó como proyecto algo similar a lo que nos habían dejado a nosotros, y al igual que nuestro maestro, también dejó material que podría servir de inspiración y con ello habían proyectos de generaciones pasadas del curso, articulos, proyectos de github, entre otros. La cantidad de diferentes proyectos que había en esta página eran demasiados, pero entre ellos encontré un proyecto de &lt;a href=&#34;http://cs231n.stanford.edu/reports/2017/pdfs/403.pdf&#34; target=&#34;_blank&#34;&gt;coloración automática de bosquejos&lt;/a&gt; donde, en resumen, su red lo que hacía era que a base de un bosquejo o dibujo de algún paisaje, la red colorea el dibujo. Este proyecto se basó en un modelo de red neuronal llamado &lt;a href=&#34;https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix&#34; target=&#34;_blank&#34;&gt;pix2pix&lt;/a&gt; donde tiene su propio artículo llamado &lt;a href=&#34;https://arxiv.org/pdf/1611.07004.pdf&#34; target=&#34;_blank&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt; el cual tiene su propio artículo llamado &lt;a href=&#34;https://arxiv.org/pdf/1611.07004.pdf&#34; target=&#34;_blank&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;modelo-pix2pix&#34;&gt;Modelo pix2pix&lt;/h2&gt;

&lt;p&gt;El modelo &lt;strong&gt;&lt;em&gt;pix2pix&lt;/em&gt;&lt;/strong&gt; es una red generativa antagónica (&lt;a href=&#34;https://en.wikipedia.org/wiki/Generative_adversarial_network&#34; target=&#34;_blank&#34;&gt;GAN&lt;/a&gt;) el cual tiene como intención ser una red generadora de imágenes de propósito general, donde dependiendo de qué conjunto de imágenes se use para entrenar la red, esta puede producir una imagen de salida a partir de una imagen de entrada y esta imagen de salida es parecida al conjunto de imágenes de salida con el que se entrenó. Y la manera en que funciona es que en realidad se usan dos redes neuronales, una red &lt;strong&gt;&lt;em&gt;discriminativa&lt;/em&gt;&lt;/strong&gt; y una red &lt;strong&gt;&lt;em&gt;generativa&lt;/em&gt;&lt;/strong&gt;, donde la red generativa trata de producir imágenes par a engañar a la red discriminativa pero el objetivo de esta última red es de aprender a distinguir entre una imagen real y una imagen falsa que haya hecho la red generativa. Entonces el proceso de aprendizaje del modelo pix2pix es básicamente hacer que la red generativa genere imágenes lo más cercano posible a las imágenes reales y la red discriminativa se vuelva mejor diferenciando entre las imágenes reales y las que genere la red generativa. Cuando se termine de entrenar la red del modelo pix2pix solo se usará la red generativa, pues el punto de todo esto es hacer una red que genere imágenes a partir de otra imagen.&lt;/p&gt;

&lt;p&gt;Las arquitecturas que usan las dos redes de pix2pix son:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Red generativa: El modelo propone usar dos tipos de arquitecturas: &lt;a href=&#34;https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;&lt;em&gt;encoder-decoder&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt; y &lt;a href=&#34;https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;&lt;em&gt;U-Net&lt;/em&gt;&lt;/strong&gt;&lt;/a&gt;. En una red convolucional consiste en hacer una convolución (downsampling o submuestreo) tras convolución a la imagen usando métodos como max pooling para encontrar los valores más importantes en un grupo de pixeles de la imagen y asi poder generalizar la información que esta en una imagen no importando en que parte de la imagen este dicha información. Entonces, lo que propone pix2pix es que una vez submuestreado la imagen de entrada lo más que se desea, se hace lo contrario al convolución y se hace una convolucion para atras (&lt;em&gt;convolución transpuesta&lt;/em&gt; o &lt;em&gt;deconvolución&lt;/em&gt; (este último termino se recomienda no usarlo ya que es usada en otras áreas de las matemáticas o visión de computadoras)) hasta llegar a las dimensiones de la imagen original de entrada. Pero esta idea se ha mejorado usando una modificación llamada &lt;a href=&#34;https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;encoder-decoder&lt;/em&gt;&lt;/a&gt; que es una arquitectura relativamente nueva del 2014 (no tanto para el área del aprendizaje profundo que es un área que se desarrolla muy rápido) el cual propone traer información de la imagen en un &amp;ldquo;punto del tiempo&amp;rdquo; de cuando estabamos convolucionando la imagen a otro &amp;ldquo;punto del tiempo&amp;rdquo; de cuando estamos convolucionando para atras cuando este tiene la misma dimensión que la imagen de donde estamos trayendo información, esto se puede ver algo así:&lt;/li&gt;
&lt;/ul&gt;




&lt;figure&gt;

&lt;img src=&#34;encoder_decoder.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figura &#34; data-post=&#34;:&#34; &gt;
  &lt;h4&gt;Diagrama de la arquitectura encoder_decoder&lt;/h4&gt;
  
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;mi-modelo-sketch2shiba&#34;&gt;Mi modelo sketch2shiba&lt;/h2&gt;
</description>
    </item>
    
  </channel>
</rss>
