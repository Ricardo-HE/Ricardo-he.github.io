<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sesgo on Ricardo Holguin</title>
    <link>https://ricardohe97.github.io/tags/sesgo/</link>
    <description>Recent content in Sesgo on Ricardo Holguin</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>es</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Tue, 27 Nov 2018 14:28:55 -0700</lastBuildDate>
    
	<atom:link href="https://ricardohe97.github.io/tags/sesgo/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Teoría De Clasificadores</title>
      <link>https://ricardohe97.github.io/post/teoria-de-clasificadores/</link>
      <pubDate>Tue, 27 Nov 2018 14:28:55 -0700</pubDate>
      
      <guid>https://ricardohe97.github.io/post/teoria-de-clasificadores/</guid>
      <description>Dimensión VC La dimensión VC es definido por Vapnik y Chervonenkis, quienes dicen que cada clasificador tiene una dimension VC, el cual es la cardinalidad (tamaño) del conjunto más grande de datos que el clasificador pueda desentrañar.
Es importante entender la importancia práctica de la dimensión VC. En muchos casos, el número exacto de la dimensión VC de un clasificador no es importante. Más bien, es usado más para clasificar diferentes tipos de algoritmos por su complejidad; por ejemplo, la clase de clasificadores simples podría incluir figuras basicas como lineas, circulos, o rectangulos, donde la clase de clasificadores complejos puede incluir clasificadores como perceptron de multiples capas, potenciación del gradiente, u otro clasificador no lineal.</description>
    </item>
    
  </channel>
</rss>