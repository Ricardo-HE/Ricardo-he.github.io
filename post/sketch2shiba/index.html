<!DOCTYPE html>
<html lang="es">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.1.0">
  <meta name="generator" content="Hugo 0.50" />
  <meta name="author" content="Ricardo Holguin Esquer">

  
  
  
  
    
  
  <meta name="description" content="Red GAN que a partir de un bosquejo de un perro, esta genera una imagen de un perro shiba inu.">

  
  <link rel="alternate" hreflang="es" href="https://ricardohe97.github.io/post/sketch2shiba/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="hsl(339, 90%, 68%)">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/dracula.min.css" crossorigin="anonymous">
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  

  
  
  

  
  <link rel="alternate" href="https://ricardohe97.github.io/index.xml" type="application/rss+xml" title="Ricardo Holguin">
  <link rel="feed" href="https://ricardohe97.github.io/index.xml" type="application/rss+xml" title="Ricardo Holguin">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://ricardohe97.github.io/post/sketch2shiba/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@RichieGCC">
  <meta property="twitter:creator" content="@RichieGCC">
  
  <meta property="og:site_name" content="Ricardo Holguin">
  <meta property="og:url" content="https://ricardohe97.github.io/post/sketch2shiba/">
  <meta property="og:title" content="Sketch2shiba | Ricardo Holguin">
  <meta property="og:description" content="Red GAN que a partir de un bosquejo de un perro, esta genera una imagen de un perro shiba inu.">
  
  
    
  <meta property="og:image" content="https://ricardohe97.github.io/img/icon-192.png">
  <meta property="og:locale" content="es">
  
  <meta property="article:published_time" content="2018-11-09T11:18:37-07:00">
  
  <meta property="article:modified_time" content="2018-11-09T11:18:37-07:00">
  

  

  

  <title>Sketch2shiba | Ricardo Holguin</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" class="dark">

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Ricardo Holguin</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Barra de navegaciÃ³n">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Inicio</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#contact">
            
            <span>Contacto</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  













<div class="article-header d-xl-none">
  <img src="/post/sketch2shiba/featured_hub423ce1c045cde763c5197f0d065bb42_36825_1600x400_fill_q90_box_smart1.jpg" class="article-banner" itemprop="image" alt="">
  
</div>


<div class="container-fluid split-header d-none d-xl-block">
  <div class="row">
    <div class="col-6">
      <div class="split-header-content">
        <h1 itemprop="name">Sketch2shiba</h1>

        

        

<div class="article-metadata">

  
  
  
  <div>
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Ricardo Holguin Esquer</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2018-11-09 11:18:37 -0700 MST" itemprop="datePublished">
    <time datetime="2018-11-09 11:18:37 -0700 MST" itemprop="dateModified">
      Fri, 09 Nov 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Ricardo Holguin Esquer">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min de lectura
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="/categories/deep-learning/">Deep learning</a>
    
  </span>
  
  

  

</div>


        







  










        
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Sketch2shiba&amp;url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f&amp;title=Sketch2shiba"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f&amp;title=Sketch2shiba"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Sketch2shiba&amp;body=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


      </div>
    </div>
    <div class="col-6">
      <div class="split-header-image">
        <img src="/post/sketch2shiba/featured_hub423ce1c045cde763c5197f0d065bb42_36825_680x500_fill_q90_box_smart1.jpg" itemprop="image" alt="">
        
      </div>
    </div>
  </div>
</div>

<div class="article-container d-xl-none">
  <h1 itemprop="name">Sketch2shiba</h1>

  

  

<div class="article-metadata">

  
  
  
  <div>
    <span itemscope itemprop="author" itemtype="http://schema.org/Person">
      <span itemprop="name">Ricardo Holguin Esquer</span>
    </span>
    
  </div>
  

  <span class="article-date">
    
    <meta content="2018-11-09 11:18:37 -0700 MST" itemprop="datePublished">
    <time datetime="2018-11-09 11:18:37 -0700 MST" itemprop="dateModified">
      Fri, 09 Nov 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Ricardo Holguin Esquer">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min de lectura
  </span>
  

  
  

  
  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder"></i>
    
    <a href="/categories/deep-learning/">Deep learning</a>
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Sketch2shiba&amp;url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f"
         target="_blank" rel="noopener">
        <i class="fab fa-facebook-f"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f&amp;title=Sketch2shiba"
         target="_blank" rel="noopener">
        <i class="fab fa-linkedin-in"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f&amp;title=Sketch2shiba"
         target="_blank" rel="noopener">
        <i class="fab fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Sketch2shiba&amp;body=https%3a%2f%2fricardohe97.github.io%2fpost%2fsketch2shiba%2f">
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


  







  









</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<h2 id="antecedentes">Antecedentes</h2>

<p>Actualmente curso la materia de redes neuronales impartida por el profesor <a href="http://mat.uson.mx/~juliowaissman/" target="_blank">Julio Waissman</a>, y para proyecto de mitad de curso tenÃ­amos que hacer alguna red neuronal convolucional (CNN). El profesor nos orientÃ³ con la idea de basarnos en algÃºn proyecto o red ya hecha y aplicarla a otra idea que tengamos. TambiÃ©n nos mostrÃ³ algunos artÃ­culo o proyectos de <a href="https://github.com/" target="_blank">Github</a> que nos pudieran ayudar de inspiraciÃ³n. Pero ninguno de los ejemplo que nos mostrÃ³ me llamÃ³ mucho la atenciÃ³n (porque me parecÃ­an aburridos o algÃºn otro compaÃ±ero tomÃ³ algÃºn proyecto que tambiÃ©n yo querÃ­a). Entonces decidÃ­ buscar otros proyectos en internet, y fue un poco despuÃ©s de cuando decidÃ­ buscar que me encontrÃ© con una pÃ¡gina de un <a href="http://cs231n.stanford.edu/project.html" target="_blank">curso de Stanford de Redes convolucionales para reconocimiento visual</a> donde en dicho curso el profesor les dejÃ³ como proyecto algo similar a lo que nos habÃ­an dejado a nosotros, y al igual que nuestro maestro, tambiÃ©n dejÃ³ material que podrÃ­a servir de inspiraciÃ³n y con ello habÃ­an proyectos de generaciones pasadas del curso, artÃ­culos, proyectos de github, entre otros. La cantidad de diferentes proyectos que habÃ­a en esta pÃ¡gina eran demasiados, pero entre ellos encontrÃ© un proyecto de <a href="http://cs231n.stanford.edu/reports/2017/pdfs/403.pdf" target="_blank">coloraciÃ³n automÃ¡tica de bosquejos</a> donde, en resumen, su red lo que hacÃ­a era que a base de un bosquejo o dibujo de algÃºn paisaje, la red colorea el dibujo. Este proyecto se basÃ³ en un modelo de red neuronal llamado <a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank">pix2pix</a> que tiene su propio artÃ­culo llamado <a href="https://arxiv.org/pdf/1611.07004.pdf" target="_blank">Image-to-Image Translation with Conditional Adversarial Networks</a>.</p>

<h2 id="modelo-pix2pix">Modelo pix2pix</h2>

<p>El modelo <strong><em>pix2pix</em></strong> es una red generativa antagÃ³nica (<a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" target="_blank">GAN</a>) el cual tiene como intenciÃ³n ser una red generadora de imÃ¡genes de propÃ³sito general, donde dependiendo de quÃ© conjunto de imÃ¡genes se use para entrenar la red, esta puede producir una imagen de salida a partir de una imagen de entrada y esta imagen de salida es parecida al conjunto de imÃ¡genes de salida con el que se entrenÃ³.</p>

<p>La manera en que funciona es que en realidad se usan dos redes neuronales, una red <strong><em>discriminativa</em></strong> y una red <strong><em>generativa</em></strong>, donde la red generativa trata de producir imÃ¡genes par a engaÃ±ar a la red discriminativa pero el objetivo de esta Ãºltima red es de aprender a distinguir entre una imagen real y una imagen falsa que haya hecho la red generativa. Entonces el proceso de aprendizaje del modelo pix2pix es bÃ¡sicamente hacer que la red generativa genere imÃ¡genes lo mÃ¡s cercano posible a las imÃ¡genes reales y la red discriminativa se vuelva mejor diferenciando entre las imÃ¡genes reales y las que genere la red generativa. Cuando se termine de entrenar la red del modelo pix2pix sÃ³lo se usarÃ¡ la red generativa, pues el punto de todo esto es hacer una red que genere imÃ¡genes a partir de otra imagen.</p>

<p>Las arquitecturas que usan las dos redes de pix2pix son:</p>

<ul>
<li><p>Red generativa: El modelo propone usar dos tipos de arquitecturas: <a href="https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf" target="_blank"><strong><em>encoder-decoder</em></strong></a> y <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" target="_blank"><strong><em>U-Net</em></strong></a>.</p>

<p>En una red convolucional consiste en hacer una convoluciÃ³n (downsampling o submuestreo) tras convoluciÃ³n a la imagen usando mÃ©todos como max pooling para encontrar los valores mÃ¡s importantes en un grupo de pÃ­xeles de la imagen y asÃ­ poder generalizar la informaciÃ³n que estÃ¡ en una imagen no importando en que parte de la imagen estÃ© dicha informaciÃ³n. Entonces, lo que propone pix2pix es que una vez submuestreado la imagen de entrada lo mÃ¡s que se desea, se hace lo contrario al convoluciÃ³n y se hace una convoluciÃ³n para atrÃ¡s (<em>convoluciÃ³n transpuesta</em> o <em>deconvoluciÃ³n</em> (este Ãºltimo tÃ©rmino se recomienda no usarlo ya que es usada en otras Ã¡reas de las matemÃ¡ticas o visiÃ³n de computadoras)) hasta llegar a las dimensiones de la imagen original de entrada.</p>

<p>Esta idea se ha mejorado usando una modificaciÃ³n llamada <strong><em>encoder-decoder</em></strong> que es una arquitectura relativamente nueva del 2014 (no tanto para el Ã¡rea del aprendizaje profundo que es un Ã¡rea que se desarrolla muy rÃ¡pido) el cual propone traer informaciÃ³n de la imagen en un &ldquo;punto del tiempo&rdquo; de cuando estÃ¡bamos convolucionando la imagen a otro &ldquo;punto del tiempo&rdquo; de cuando estamos convolucionando para atrÃ¡s cuando este tiene la misma dimensiÃ³n que la imagen de donde estamos trayendo informaciÃ³n, esto se puede ver mejor en la figura 1. De esta manera la informaciÃ³n de las imÃ¡genes convolucionadas nos indican los objetos que hay en una imagen, y al transferir la informaciÃ³n de momentos antes de convolucionar la imagen al final de donde estamos haciendo convoluciÃ³n atrÃ¡s se puede obtener informaciÃ³n de donde estÃ¡ dicho objeto. Es como tener lo mejor de dos mundos. Esta es mÃ¡s o menos la idea del encoder-decoder.</p>

<p><strong><em>U-Net</em></strong> es casi lo mismo que el encoder-decoder. La diferencia de esta es que encoder-decoder hace la transferencia de informaciÃ³n en <strong>algunos puntos o capas</strong> de las etapas del proceso de convolucionar la imagen original al final de la convoluciÃ³n (cuando tenemos la imagen en una resoluciÃ³n muy pequeÃ±a) y U-Net lo hace en <strong>cada capa del proceso</strong>, de tal manera donde cada capa $i$ y $n-i$ estÃ¡n conectados, donde $n$ es el nÃºmero total de capas. Cada conexiÃ³n simplemente concatena todos los canales de la capa $i$ con los de la capa $n-i$. La arquitectura de la U-Net se ve como en la figura 2. Por la importancia de los datos de la imagen de entrada con la imagen de salida se optÃ³ por usar U-Net como red generativa para el modelo.</p></li>
</ul>




<figure>

<img src="encoder_decoder.png" />



<figcaption data-pre="Figura " data-post=":" class="numbered">
  <h4>Diagrama de una arquitectura encoder_decoder</h4>
  
</figcaption>

</figure>




<figure>

<img src="unet.png" />



<figcaption data-pre="Figura " data-post=":" class="numbered">
  <h4>Diagrama de una arquitectura U-Net</h4>
  
</figcaption>

</figure>

<ul>
<li><p>Red discriminativa: Para el modelo pix2pix se diseÃ±Ã³ una arquitectura llamada <em>PatchGAN</em>, la cual solo penaliza la estructura de la imagen a escala de parches. Este discriminador trata de clasificar si cada parce $N \times N $ en una imagen es real o es falsa. El discriminador se ejecuta en cada convoluciÃ³n de la imagen, promediando todos los resultados para producir una salida final $D$. En el artÃ­culo original de pix2pix demuestran que $N$ puede ser mucho mÃ¡s pequeÃ±o que el tamaÃ±o de la imagen completa y aun asÃ­ producir imÃ¡genes de alta calidad, y esto es ventajoso ya que esto produce menos parÃ¡metros, ejecuta mÃ¡s rÃ¡pido, y puede ser aplicado arbitrariamente a imÃ¡genes grandes.</p>

<p>Lo anterior indica que se puede modelar la imagen como un <a href="https://en.wikipedia.org/wiki/Markov_random_field" target="_blank">campo aleatorio de Markov</a>, asumiendo independiencia entre pÃ­xeles separados por mÃ¡s de parche de diÃ¡metro. Este tipo de suposiciones son usadas en modelos de texturas y estilo. Por lo que <em>PatchGan</em> puede ser entendido como una forma de pÃ©rdida de textura/stilo.</p>

<p>Este discriminador usa una funciÃ³n de pÃ©rdida $L1$ porque esta produce resultados donde la imagen no sale tan borrosa como $L2$ y hacen que la imagen producida tenga una figura parecida a la imagen de entrada, pero aun asi $L1$ produce imÃ¡genes borrosas.</p></li>
</ul>

<h2 id="mi-modelo-sketch2shiba">Mi modelo sketch2shiba</h2>

<p>BasÃ¡ndome en los proyectos que he visto como lo son el <a href="http://cs231n.stanford.edu/reports/2017/pdfs/403.pdf" target="_blank">trabajo de los estudiantes de Satanford</a> y <a href="https://github.com/zaidalyafeai/zaidalyafeai.github.io/tree/master/pix2pix" target="_blank">edges2cats</a>, decidÃ­ por hacer algo similar pero con mi propia base de datos.</p>

<p>Mi proyecto se puede encontrar en <a href="https://github.com/RicardoHE97/sketch2shiba" target="_blank">sketch2shiba</a>. En este repositorio tiene la base de datos y scripts para entrenar el modelo. Actualmente no he encontrado manera de almacenar mi modelo en alguna pagina para que pudiera ser descargada de manera gratuita (los modelos son un poco pesados y plataformas como Github solo dejan que un repositorio tenga un espacio de 100MB).</p>

<h3 id="bÃºsqueda-de-ideas">BÃºsqueda de ideas</h3>

<p>TenÃ­a algunas de que hacer, pero el problema surgÃ­a de donde iba a encontrar los datos con los cuales entrenar mi red. Hay varias bases de datos como ImageNet, CIFAR-10, o podrÃ­a hacer un script que pudiera descargar imÃ¡genes de alguna pÃ¡gina que contenga muchas imÃ¡genes de algo.</p>

<h3 id="sacando-los-bosquejos">Sacando los bosquejos</h3>

<p>Pero el otro problema es que lo que querÃ­a hacer era que a base de algÃºn bosquejo te pintara dicho bosquejo. Entonces debÃ­a encontrar alguna manera para hacer los bosquejos, y obviamente no iba a hacer los bosquejos a mano para cientos de imÃ¡genes que pensaba usar para entrenar a mi red. DespuÃ©s descubrÃ­ librerÃ­as como <a href="https://github.com/opencv/opencv" target="_blank">openCV</a> la cual tiene una funciÃ³n llamada <code>Canny</code>, que hace algo parecido a encontrar un bosquejo dada una imagen, pero cuando ingresaba una imagen a esta funciÃ³n tambiÃ©n sacaba bosquejos del fondo. Fue esto Ãºltimo que me hizo darme cuenta que debÃ­a quitar el fondo de las imÃ¡genes que querÃ­a sacarle un bosquejo, pero a esto tampoco tenÃ­a pensado hacerlo a mano a cientos de imÃ¡genes, por lo que decidÃ­ buscar alguna base de datos que ya tuviera imÃ¡genes segmentadas. Y me encontrÃ© con una <a href="http://www.robots.ox.ac.uk/~vgg/data/pets/" target="_blank">base de datos de la Universidad de Oxford</a> donde tienen una base de datos de imÃ¡genes de mascotas (perros y gatos) ya segmentados.</p>

<p>Esto fue un gran alivio porque no encontraba muchas bases de datos que te proporcionen esto, o al menos no para algo especÃ­fico como un solo tema. Otra cosa que podÃ­a sacarle de esto es que podrÃ­a usar las imÃ¡genes segmentadas como los bosquejos (mÃ¡s adelante en el blog explicare porque hice esto).</p>

<p>Ya encontrados una base de datos que tuvieran imÃ¡genes segmentadas, solo quedaba hacer un script que usando una imagen de segmentaciÃ³n (estas imÃ¡genes tienen la misma resoluciÃ³n de la imagen original y cada pixel tiene 3 valores: 1-primer plano, 2-fondo, 3-Sin clasificacion) haga que los pÃ­xeles que sean el fondo de la imagen se vuelvan de color blanco. Asi entonces quedaban imÃ¡genes parecidas a esta:</p>




<figure>

<img src="shiba_inu_24_dual.jpg" />



<figcaption data-pre="Figura " data-post=":" class="numbered">
  <h4>Imagen de segmentaciÃ³n en la izquierda e imagen original sin fondo a la derecha</h4>
  
</figcaption>

</figure>

<h3 id="decidir-sobre-que-red-hacer">decidir sobre que red hacer</h3>

<p>TenÃ­a una base de datos que tenÃ­a varÃ­as clases como gatos o perros. O podrÃ­a ser mÃ¡s especÃ­fico con la raza de los gatos o de los perros. Pero no podÃ­a tomar solo una clase general como solo perros o gatos para hacer una red sobre uno de ellos y ya.</p>

<p>La razÃ³n era mÃ¡s personal que tÃ©cnica (aÃºn asÃ­ habÃ­an razones tÃ©cnicas). No querÃ­a hacer una de gatos porque ya existÃ­a una y querÃ­a hacer algo diferente. Entonces quise hacer una de perros pero hay tantas razas de perros y muchas son muy diferentes de unas de otras y desconfiaba si al final la red podrÃ­a pintar con tantas diferentes clases. Al final decidÃ­ por solo escoger una raza, y la raza que escogÃ­ fue <strong>shiba inu</strong>.</p>

<p>La razÃ³n por la que escogÃ­ esa raza es simplemente gusto personal a dicho perro.</p>

<p>El nÃºmero de imÃ¡genes que habÃ­a de tenÃ­a al final eran 200. Este nÃºmero es muy poco para una red neuronal, normalmente se usan miles, decenas de miles, o millones de imÃ¡genes para entrenar una red, pero me inspiraba confianza que en el artÃ­culo original de pix2pix decÃ­an que con solo 400 imÃ¡genes obtenÃ­an resultados satisfactorios. Por lo que no me sentÃ­ desalentado al solo tener 200 imÃ¡genes.</p>

<h3 id="entrenar-la-red-y-problemas-de-cÃ³mputo">Entrenar la red y problemas de cÃ³mputo</h3>

<p>Para entrenar la red sol seguÃ­ la guia que habia en el repositorio de Github del modelo pix2pix. Donde solo te piden que ingreses tu base de datos en un directorio, separes que imÃ¡genes usarÃ¡s para entrenar, probar y validar. DespuÃ©s solo se corre un script con los parÃ¡metros que se quiere para entrenar tu modelo,  y Â¡listo!. Ya tienes tu red que pinta bosquejos de shiba inus&hellip; o eso quisiera.</p>

<p>La cruda verdad es que entrenar una red neuronal requiere mucho poder de cÃ³mputo y si no se tiene tal poder de cÃ³mputo entonces entrenar una red tarda mucho tiempo. Dependiendo de la red, tamaÃ±o de las imÃ¡genes, nÃºmero de imÃ¡genes, entrenar una red tarda desde horas hasta dÃ­as. Y se requiere entrenar una red varias veces para probar diferentes parÃ¡metros de entrenamiento y diferentes arquitecturas de redes. Esto con la finalidad de tener la red mÃ¡s Ã³ptima posible.</p>

<p>La Universidad de Sonora cuenta con un Ã¡rea de supercÃ³mputo, que por alguna razÃ³n su Ãºnico fuerte es que tienen cientos de procesadores, lo cual no es malo, seguro son Ãºtiles para muchos investigadores y maestros. Pero un procesador no es apto para hacer cÃ¡lculos que hace una red neuronal. Una red neuronal hace muchas operaciones matriciales y existen herramientas que son especiales para eso que se llaman unidad de procesamiento grÃ¡fico o GPU. Estas son altamente aprovechadas por los frameworks de redes neuronales como Tensorflow, Torch, Caffe, entre otros. Lamentablemente la Unison no cuenta con muchos GPUs. Solo cuenta con 4 GPUs que no pueden ejecutar las librerÃ­as requeridas por estos frameworks. TambiÃ©n cuentan con un GPU relativamente moderno y mÃ¡s poderoso (GTX 970) que fue donado recientemente por un profesor, pero es solo uno. En el curso de Redes Neuronales somos alrededor de 9, y entrenar una red neuronal a cada uno le tomaba 1 dÃ­a. TambiÃ©n tenÃ­amos la fortuna que un profesor de nuestra carrera tenÃ­a una computadora con un GPU (un poco vieja e inferior que la GTX 970 pero podÃ­a correr las librerÃ­as que necesitabamos) y decidiÃ³ hacer de esa computadora en un pequeÃ±o servidor donde nosotros podÃ­amos conectarnos para poder dejar entrenando nuestras redes.</p>

<p>Estas condiciones que tuvimos hicieron difÃ­cil poder conseguir resultados satisfactorios, o al menos a mi porque a diferencia de casi todos mis compaÃ±eros, yo no tenÃ­a una computadora con GPU en mi casa y dependÃ­a de la computadora que tenÃ­amos en la carrera y muchas veces siempre lo estaba usando algÃºn otro compaÃ±ero.</p>

<p>Al final pude entrenar mi red con 2 diferentes arquitectura de redes: resnet_9blocks y u-net_128. Con imÃ¡genes de una resoluciÃ³n de 254x254.</p>

<h2 id="resultados">Resultados</h2>

<p>ProbÃ© dos arquitecturas diferentes para la la red generativa. Una fue resnet 9 blocks y la otra U-Net 128</p>

<h3 id="red-generativa-resnet-9-blocks">Red generativa: Resnet 9 blocks</h3>

<table>
<thead>
<tr>
<th>Bosquejo</th>
<th>Imagen generada</th>
<th>Imagen real</th>
</tr>
</thead>

<tbody>
<tr>
<td>


<figure>

<img src="resnet9_201_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_201_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_201_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="resnet9_202_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_202_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_202_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="resnet9_207_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_207_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="resnet9_207_dual_real_B.png" />


</figure></td>
</tr>
</tbody>
</table>

<h3 id="red-generativa-u-net-128">Red generativa: U-Net 128</h3>

<table>
<thead>
<tr>
<th>Bosquejo</th>
<th>Imagen generada</th>
<th>Imagen real</th>
</tr>
</thead>

<tbody>
<tr>
<td>


<figure>

<img src="unet128_202_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet128_202_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet128_202_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="unet128_206_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet128_206_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet128_206_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="unet128_207_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet128_207_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet128_207_dual_real_B.png" />


</figure></td>
</tr>
</tbody>
</table>

<h3 id="red-generativa-u-net-254">Red generativa: U-Net 254</h3>

<table>
<thead>
<tr>
<th>Bosquejo</th>
<th>Imagen generada</th>
<th>Imagen real</th>
</tr>
</thead>

<tbody>
<tr>
<td>


<figure>

<img src="unet254_208_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet254_208_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet254_208_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="unet254_209_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet254_209_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet254_209_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="unet254_210_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet254_210_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet254_210_dual_real_B.png" />


</figure></td>
</tr>

<tr>
<td>


<figure>

<img src="unet254_212_dual_real_A.png" />


</figure></td>
<td>


<figure>

<img src="unet254_212_dual_fake_B.png" />


</figure></td>
<td>


<figure>

<img src="unet254_212_dual_real_B.png" />


</figure></td>
</tr>
</tbody>
</table>

<p>Los resultados no fueron lo que esperaba, pero se debe principalmente a que solo usaba el contorno de los perros como bosquejo y al utilizar muy pocas imÃ¡genes la red no puede generalizar tan bien.</p>

<p>AquÃ­ es importante recordar que al inicio del blog dije que mientras buscaba como hacer los bosquejos de mis imÃ¡genes llegue a utilizar una funciÃ³n de una librerÃ­a que hacÃ­a bosquejos que se pueden asemejar con un dibujo de una imagen. Pero al final nunca lo usÃ©, y la razÃ³n es que simplemente no tuve tiempo pero es algo que proximamente hare y entrenare asÃ­ a la red.</p>

<h2 id="conclusiÃ³n">ConclusiÃ³n</h2>

<p>Esta experiencia me ha enseÃ±ado las partes fÃ¡ciles y difÃ­ciles del Ã¡rea de aprendizaje profundo. Yo sentÃ­ que la parte que lleva mÃ¡s tiempo es preprocesar los datos (al menos en este caso donde tuve que encontrar imÃ¡genes, sacarle bosquejo, quitar fondo, pegarlas en una sola imagen, escalar y recortar imÃ¡genes). Ya al momento de diseÃ±ar una red me baso mucho en redes que ya han sido hechas y han demostrado que funcionan en aplicaciones especÃ­ficas.</p>

<h2 id="continuaciÃ³n">ContinuaciÃ³n</h2>

<p>Planeo mejorar esta red utilizando imÃ¡genes de mÃ¡s alta resoluciÃ³n y usar imÃ¡genes que se parezcan mÃ¡s a bosquejos que las que utiliza primeramente.</p>

    </div>

    


<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep learning</a>
  
  <a class="badge badge-light" href="/tags/cnn/">CNN</a>
  
  <a class="badge badge-light" href="/tags/machine-learning/">Machine learning</a>
  
  <a class="badge badge-light" href="/tags/dogs/">dogs</a>
  
  <a class="badge badge-light" href="/tags/ia/">IA</a>
  
  <a class="badge badge-light" href="/tags/artificial-inteligence/">Artificial inteligence</a>
  
</div>



    



  




<div class="media author-card" itemscope itemtype="http://schema.org/Person">
  
  <img class="portrait mr-3" src="/img/portrait.jpg" itemprop="image" alt="Avatar">
  
  <div class="media-body">
    <h5 class="card-title" itemprop="name"><a href="/">Ricardo Holguin Esquer</a></h5>
    <h6 class="card-subtitle">Estudiante de Ciencias de la computaciÃ³n</h6>
    
    <ul class="network-icon" aria-hidden="true">
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="/rholguinesquer@gmail.com" >
          <i class="fas fa-envelope"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://twitter.com/RichieGCC" target="_blank" rel="noopener">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
      
      
      
      
        
      
      
      
      
      
        
      
      <li>
        <a itemprop="sameAs" href="https://github.com/RicardoHE97" target="_blank" rel="noopener">
          <i class="fab fa-github"></i>
        </a>
      </li>
      
    </ul>
  </div>
</div>




    
    

    

    


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Citar</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copiar
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Descargar
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

    

    
    
    
    
    <script src="/js/mathjax-config.500a6cbb2c0f345fcecc21b3116d6637aa78f1f11db8081ea581abe05390c2e8f3bbffe61be3cf0217baf785c40efceabe51050a4f007e69af94efd3643260e8.js" integrity="sha512-UApsuywPNF/OzCGzEW1mN6p48fEduAgepYGr4FOQwujzu//mG&#43;PPAhe694XEDvzqvlEFCk8AfmmvlO/TZDJg6A=="></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js" integrity="sha256-/BfiIkHlHoVihZdc6TFuj7MmJ0TWcWsMXkeDFwhi0zw=" crossorigin="anonymous"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    <script src="/js/hugo-academic.js"></script>
    

    
    

    
    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "",
        'results': "",
        'no_results': ""
      };
      const content_type = {
        'post': "Posts",
        'project': "Proyectos",
        'publication' : "Publicaciones",
        'talk' : "Conferencias"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    <script src="/js/search.js"></script>
    

    
    

  </body>
</html>

