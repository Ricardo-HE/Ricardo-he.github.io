[
  {
    "authors": [
      "Ricardo Holguin Esquer"
    ],
    "categories": [
      "Deep learning"
    ],
    "content": " Antecedentes Actualmente curso la materia de redes neuronales impartida por el profesor Julio Waissman, y para proyecto de mitad de curso teníamos que hacer alguna red neuronal convolucional (CNN). El profesor nos orientó con la idea de basarnos en algún proyecto o red ya hecha y aplicarla a otra idea que tengamos. También nos mostró algunos artículo o proyectos de Github que nos pudieran ayudar de inspiración. Pero ninguno de los ejemplo que nos mostró me llamó mucho la atención (porque me parecían aburridos o algún otro compañero tomó algún proyecto que también yo quería). Entonces decidí buscar otros proyectos en internet, y fue un poco después de cuando decidí buscar que me encontré con una página de un curso de Stanford de Redes convolucionales para reconocimiento visual donde en dicho curso el profesor les dejó como proyecto algo similar a lo que nos habían dejado a nosotros, y al igual que nuestro maestro, también dejó material que podría servir de inspiración y con ello habían proyectos de generaciones pasadas del curso, artículos, proyectos de github, entre otros. La cantidad de diferentes proyectos que había en esta página eran demasiados, pero entre ellos encontré un proyecto de coloración automática de bosquejos donde, en resumen, su red lo que hacía era que a base de un bosquejo o dibujo de algún paisaje, la red colorea el dibujo. Este proyecto se basó en un modelo de red neuronal llamado pix2pix que tiene su propio artículo llamado Image-to-Image Translation with Conditional Adversarial Networks.\nModelo pix2pix El modelo pix2pix es una red generativa antagónica (GAN) el cual tiene como intención ser una red generadora de imágenes de propósito general, donde dependiendo de qué conjunto de imágenes se use para entrenar la red, esta puede producir una imagen de salida a partir de una imagen de entrada y esta imagen de salida es parecida al conjunto de imágenes de salida con el que se entrenó.\nLa manera en que funciona es que en realidad se usan dos redes neuronales, una red discriminativa y una red generativa, donde la red generativa trata de producir imágenes par a engañar a la red discriminativa pero el objetivo de esta última red es de aprender a distinguir entre una imagen real y una imagen falsa que haya hecho la red generativa. Entonces el proceso de aprendizaje del modelo pix2pix es básicamente hacer que la red generativa genere imágenes lo más cercano posible a las imágenes reales y la red discriminativa se vuelva mejor diferenciando entre las imágenes reales y las que genere la red generativa. Cuando se termine de entrenar la red del modelo pix2pix sólo se usará la red generativa, pues el punto de todo esto es hacer una red que genere imágenes a partir de otra imagen.\nLas arquitecturas que usan las dos redes de pix2pix son:\n Red generativa: El modelo propone usar dos tipos de arquitecturas: encoder-decoder y U-Net.\nEn una red convolucional consiste en hacer una convolución (downsampling o submuestreo) tras convolución a la imagen usando métodos como max pooling para encontrar los valores más importantes en un grupo de píxeles de la imagen y así poder generalizar la información que está en una imagen no importando en que parte de la imagen esté dicha información. Entonces, lo que propone pix2pix es que una vez submuestreado la imagen de entrada lo más que se desea, se hace lo contrario al convolución y se hace una convolución para atrás (convolución transpuesta o deconvolución (este último término se recomienda no usarlo ya que es usada en otras áreas de las matemáticas o visión de computadoras)) hasta llegar a las dimensiones de la imagen original de entrada.\nEsta idea se ha mejorado usando una modificación llamada encoder-decoder que es una arquitectura relativamente nueva del 2014 (no tanto para el área del aprendizaje profundo que es un área que se desarrolla muy rápido) el cual propone traer información de la imagen en un \u0026ldquo;punto del tiempo\u0026rdquo; de cuando estábamos convolucionando la imagen a otro \u0026ldquo;punto del tiempo\u0026rdquo; de cuando estamos convolucionando para atrás cuando este tiene la misma dimensión que la imagen de donde estamos trayendo información, esto se puede ver mejor en la figura 1. De esta manera la información de las imágenes convolucionadas nos indican los objetos que hay en una imagen, y al transferir la información de momentos antes de convolucionar la imagen al final de donde estamos haciendo convolución atrás se puede obtener información de donde está dicho objeto. Es como tener lo mejor de dos mundos. Esta es más o menos la idea del encoder-decoder.\nU-Net es casi lo mismo que el encoder-decoder. La diferencia de esta es que encoder-decoder hace la transferencia de información en algunos puntos o capas de las etapas del proceso de convolucionar la imagen original al final de la convolución (cuando tenemos la imagen en una resolución muy pequeña) y U-Net lo hace en cada capa del proceso, de tal manera donde cada capa $i$ y $n-i$ están conectados, donde $n$ es el número total de capas. Cada conexión simplemente concatena todos los canales de la capa $i$ con los de la capa $n-i$. La arquitectura de la U-Net se ve como en la figura 2. Por la importancia de los datos de la imagen de entrada con la imagen de salida se optó por usar U-Net como red generativa para el modelo.\n   Diagrama de una arquitectura encoder_decoder    Diagrama de una arquitectura U-Net    Red discriminativa: Para el modelo pix2pix se diseñó una arquitectura llamada PatchGAN, la cual solo penaliza la estructura de la imagen a escala de parches. Este discriminador trata de clasificar si cada parce $N \\times N $ en una imagen es real o es falsa. El discriminador se ejecuta en cada convolución de la imagen, promediando todos los resultados para producir una salida final $D$. En el artículo original de pix2pix demuestran que $N$ puede ser mucho más pequeño que el tamaño de la imagen completa y aun así producir imágenes de alta calidad, y esto es ventajoso ya que esto produce menos parámetros, ejecuta más rápido, y puede ser aplicado arbitrariamente a imágenes grandes.\nLo anterior indica que se puede modelar la imagen como un campo aleatorio de Markov, asumiendo independiencia entre píxeles separados por más de parche de diámetro. Este tipo de suposiciones son usadas en modelos de texturas y estilo. Por lo que PatchGan puede ser entendido como una forma de pérdida de textura/stilo.\nEste discriminador usa una función de pérdida $L1$ porque esta produce resultados donde la imagen no sale tan borrosa como $L2$ y hacen que la imagen producida tenga una figura parecida a la imagen de entrada, pero aun asi $L1$ produce imágenes borrosas.\n  Mi modelo sketch2shiba Basándome en los proyectos que he visto como lo son el trabajo de los estudiantes de Satanford y edges2cats, decidí por hacer algo similar pero con mi propia base de datos.\nMi proyecto se puede encontrar en sketch2shiba. En este repositorio tiene la base de datos y scripts para entrenar el modelo. Actualmente no he encontrado manera de almacenar mi modelo en alguna pagina para que pudiera ser descargada de manera gratuita (los modelos son un poco pesados y plataformas como Github solo dejan que un repositorio tenga un espacio de 100MB).\nBúsqueda de ideas Tenía algunas de que hacer, pero el problema surgía de donde iba a encontrar los datos con los cuales entrenar mi red. Hay varias bases de datos como ImageNet, CIFAR-10, o podría hacer un script que pudiera descargar imágenes de alguna página que contenga muchas imágenes de algo.\nSacando los bosquejos Pero el otro problema es que lo que quería hacer era que a base de algún bosquejo te pintara dicho bosquejo. Entonces debía encontrar alguna manera para hacer los bosquejos, y obviamente no iba a hacer los bosquejos a mano para cientos de imágenes que pensaba usar para entrenar a mi red. Después descubrí librerías como openCV la cual tiene una función llamada Canny, que hace algo parecido a encontrar un bosquejo dada una imagen, pero cuando ingresaba una imagen a esta función también sacaba bosquejos del fondo. Fue esto último que me hizo darme cuenta que debía quitar el fondo de las imágenes que quería sacarle un bosquejo, pero a esto tampoco tenía pensado hacerlo a mano a cientos de imágenes, por lo que decidí buscar alguna base de datos que ya tuviera imágenes segmentadas. Y me encontré con una base de datos de la Universidad de Oxford donde tienen una base de datos de imágenes de mascotas (perros y gatos) ya segmentados.\nEsto fue un gran alivio porque no encontraba muchas bases de datos que te proporcionen esto, o al menos no para algo específico como un solo tema. Otra cosa que podía sacarle de esto es que podría usar las imágenes segmentadas como los bosquejos (más adelante en el blog explicare porque hice esto).\nYa encontrados una base de datos que tuvieran imágenes segmentadas, solo quedaba hacer un script que usando una imagen de segmentación (estas imágenes tienen la misma resolución de la imagen original y cada pixel tiene 3 valores: 1-primer plano, 2-fondo, 3-Sin clasificacion) haga que los píxeles que sean el fondo de la imagen se vuelvan de color blanco. Asi entonces quedaban imágenes parecidas a esta:\n Imagen de segmentación en la izquierda e imagen original sin fondo a la derecha   decidir sobre que red hacer Tenía una base de datos que tenía varías clases como gatos o perros. O podría ser más específico con la raza de los gatos o de los perros. Pero no podía tomar solo una clase general como solo perros o gatos para hacer una red sobre uno de ellos y ya.\nLa razón era más personal que técnica (aún así habían razones técnicas). No quería hacer una de gatos porque ya existía una y quería hacer algo diferente. Entonces quise hacer una de perros pero hay tantas razas de perros y muchas son muy diferentes de unas de otras y desconfiaba si al final la red podría pintar con tantas diferentes clases. Al final decidí por solo escoger una raza, y la raza que escogí fue shiba inu.\nLa razón por la que escogí esa raza es simplemente gusto personal a dicho perro.\nEl número de imágenes que había de tenía al final eran 200. Este número es muy poco para una red neuronal, normalmente se usan miles, decenas de miles, o millones de imágenes para entrenar una red, pero me inspiraba confianza que en el artículo original de pix2pix decían que con solo 400 imágenes obtenían resultados satisfactorios. Por lo que no me sentí desalentado al solo tener 200 imágenes.\nEntrenar la red y problemas de cómputo Para entrenar la red sol seguí la guia que habia en el repositorio de Github del modelo pix2pix. Donde solo te piden que ingreses tu base de datos en un directorio, separes que imágenes usarás para entrenar, probar y validar. Después solo se corre un script con los parámetros que se quiere para entrenar tu modelo, y ¡listo!. Ya tienes tu red que pinta bosquejos de shiba inus\u0026hellip; o eso quisiera.\nLa cruda verdad es que entrenar una red neuronal requiere mucho poder de cómputo y si no se tiene tal poder de cómputo entonces entrenar una red tarda mucho tiempo. Dependiendo de la red, tamaño de las imágenes, número de imágenes, entrenar una red tarda desde horas hasta días. Y se requiere entrenar una red varias veces para probar diferentes parámetros de entrenamiento y diferentes arquitecturas de redes. Esto con la finalidad de tener la red más óptima posible.\nLa Universidad de Sonora cuenta con un área de supercómputo, que por alguna razón su único fuerte es que tienen cientos de procesadores, lo cual no es malo, seguro son útiles para muchos investigadores y maestros. Pero un procesador no es apto para hacer cálculos que hace una red neuronal. Una red neuronal hace muchas operaciones matriciales y existen herramientas que son especiales para eso que se llaman unidad de procesamiento gráfico o GPU. Estas son altamente aprovechadas por los frameworks de redes neuronales como Tensorflow, Torch, Caffe, entre otros. Lamentablemente la Unison no cuenta con muchos GPUs. Solo cuenta con 4 GPUs que no pueden ejecutar las librerías requeridas por estos frameworks. También cuentan con un GPU relativamente moderno y más poderoso (GTX 970) que fue donado recientemente por un profesor, pero es solo uno. En el curso de Redes Neuronales somos alrededor de 9, y entrenar una red neuronal a cada uno le tomaba 1 día. También teníamos la fortuna que un profesor de nuestra carrera tenía una computadora con un GPU (un poco vieja e inferior que la GTX 970 pero podía correr las librerías que necesitabamos) y decidió hacer de esa computadora en un pequeño servidor donde nosotros podíamos conectarnos para poder dejar entrenando nuestras redes.\nEstas condiciones que tuvimos hicieron difícil poder conseguir resultados satisfactorios, o al menos a mi porque a diferencia de casi todos mis compañeros, yo no tenía una computadora con GPU en mi casa y dependía de la computadora que teníamos en la carrera y muchas veces siempre lo estaba usando algún otro compañero.\nAl final pude entrenar mi red con 2 diferentes arquitectura de redes: resnet_9blocks y u-net_128. Con imágenes de una resolución de 254x254.\nResultados Probé dos arquitecturas diferentes para la la red generativa. Una fue resnet 9 blocks y la otra U-Net 128\nRed generativa: Resnet 9 blocks    Bosquejo Imagen generada Imagen real                                       Red generativa: U-Net 128    Bosquejo Imagen generada Imagen real                                       Los resultados no fueron lo que esperaba, pero se debe principalmente a que solo usaba el contorno de los perros como bosquejo y al utilizar muy pocas imágenes la red no puede generalizar tan bien.\nAquí es importante recordar que al inicio del blog dije que mientras buscaba como hacer los bosquejos de mis imágenes llegue a utilizar una función de una librería que hacía bosquejos que se pueden asemejar con un dibujo de una imagen. Pero al final nunca lo usé, y la razón es que simplemente no tuve tiempo pero es algo que proximamente hare y entrenare así a la red.\nConclusión Esta experiencia me ha enseñado las partes fáciles y difíciles del área de aprendizaje profundo. Yo sentí que la parte que lleva más tiempo es preprocesar los datos (al menos en este caso donde tuve que encontrar imágenes, sacarle bosquejo, quitar fondo, pegarlas en una sola imagen, escalar y recortar imágenes). Ya al momento de diseñar una red me baso mucho en redes que ya han sido hechas y han demostrado que funcionan en aplicaciones específicas.\nContinuación Planeo mejorar esta red utilizando imágenes de más alta resolución y usar imágenes que se parezcan más a bosquejos que las que utiliza primeramente.\n",
    "date": 1541787517,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "es",
    "lastmod": 1541787517,
    "objectID": "3412d1625f3ae8c86e727ad1630e8160",
    "permalink": "https://ricardohe97.github.io/post/sketch2shiba/",
    "publishdate": "2018-11-09T11:18:37-07:00",
    "relpermalink": "/post/sketch2shiba/",
    "section": "post",
    "summary": "Red GAN que a partir de un bosquejo de un perro, esta genera una imagen de un perro shiba inu.",
    "tags": [
      "Deep learning",
      "CNN",
      "Machine learning",
      "dogs",
      "IA",
      "Artificial inteligence"
    ],
    "title": "Sketch2shiba",
    "type": "post"
  }
]